{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature RPC: modelization of the customer behaviour on the landing page\n",
    "Let's consider:\n",
    "- customer: a user clicking on an advertisement and arriving on the landing page of the e-commerce platform.\n",
    " \n",
    "Let's then consider the following independent variables:\n",
    "- X = the number of FCUR (fantasy currency) the customer is going to buy\n",
    "- Revenue: sum of X\n",
    "\n",
    "According to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) the bigger the population will be, the more the revenue is going to tend to a normal distribution. What's the minimum population to make this approximation? Well, I am aware that a lot of statisticians would argue, but according to google's deep learning course on udacity, according to a probability professor, and according to my personal experience, a population of 30 is the minimum to do this approximation.\n",
    "\n",
    "Concretely, in our case, we are going to train the behaviour of the customer on data points with more than 30 clicks, and we will predict on all the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Helpers definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(data):\n",
    "    # around 20% of the population. Found Manually.\n",
    "    data_train = data[data[\"Date\"] <= \"2015-03-19\"]\n",
    "    data_test = data[data[\"Date\"] > \"2015-03-19\"]\n",
    "    \n",
    "    data_train = data_train.loc[data[\"Clicks\"] >= 30]\n",
    "    data_test = data_test.loc[data[\"Clicks\"] >= 30]\n",
    "    \n",
    "    split_percentage = len(data_train) * 100 / (len(data_train) + len(data_test)) \n",
    "    print \"INFO - percentage of the data in training set: \" + str(split_percentage) + \"%\"\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(\"./sem-database.csv\")\n",
    "\n",
    "# feature extraction\n",
    "data[\"RPC\"] = data[\"Revenue\"].apply(float) / data[\"Clicks\"] \n",
    "data = data.loc[data[\"RPC\"] > 0] # We will build something special for RPC = 0\n",
    "\n",
    "# split train/test\n",
    "data_train, data_test = split_train_test(data)\n",
    "\n",
    "# delete outliers. Not cheating: deleting them only on data_train ;-)\n",
    "outliers_border = data[data.RPC != 0].RPC.quantile(.99)\n",
    "data_train = data_train.loc[data_train.RPC < outliers_border]\n",
    "print \"INFO - Outlier border: \" + str(int(outliers_border))\n",
    "\n",
    "# create model - categorical data are encoded to \" dummies \"\n",
    "X_train = pd.get_dummies(data[\"Campaign_ID\"].apply(str)).loc[data_train.index]\n",
    "y_train = data_train[\"RPC\"]\n",
    "X_test = pd.get_dummies(data[\"Campaign_ID\"].apply(str)).loc[data_test.index]\n",
    "y_test = data_test[\"RPC\"]\n",
    "\n",
    "# Train Model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "model = SGDRegressor(loss=\"squared_loss\", n_iter=50, shuffle=True, random_state=2029)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "d = mean_squared_error(y_test, predictions)\n",
    "print \"Mean squared error: \" + str(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Graphical evaluation\n",
    "Visualisation of the discrepancy between expected and predited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dark_blue = \"#3333FF\"\n",
    "light_blue = \"#AAAAFF\"\n",
    "\n",
    "distance = pd.DataFrame()\n",
    "distance[\"expected\"] = y_test\n",
    "distance[\"predicted\"] = predictions\n",
    "\n",
    "sample_size = 30\n",
    "visualized_distance = distance.sample(n=sample_size)\n",
    "plt.scatter(range(sample_size), visualized_distance[\"expected\"], color=dark_blue)\n",
    "plt.scatter(range(sample_size), visualized_distance[\"predicted\"], color=light_blue)\n",
    "\n",
    "# Graph formattage\n",
    "plt.xlabel(\"Sample (ID)\")\n",
    "plt.ylabel(\"RPC\")\n",
    "plt.title(\"Digging into model: difference estimated vs expected\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](To display image on github)\n",
    "<img src=\"https://user-images.githubusercontent.com/1684807/28826438-757b1caa-76ca-11e7-899d-98890dec49de.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
